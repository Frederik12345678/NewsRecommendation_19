{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import polars as pl # used to read the .parquet files so its important\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add the parent directory (or specify path to 'utils' if it's higher)\n",
    "dir = os.path.abspath(os.path.join(os.getcwd(), '..', '..')) \n",
    "sys.path.append(dir)\n",
    "\n",
    "from dataloader.NRMSdataloader import NRMSDataLoader\n",
    "from models.NRMS_tensorflow import NRMSModel\n",
    "from eval.metricEval import MetricEvaluator, AucScore, MrrScore, NdcgScore\n",
    "\n",
    "# Now you can import from utils\n",
    "from utils import add_known_user_column, add_prediction_scores\n",
    "from utils import get_transformers_word_embeddings, concat_str_columns,convert_text2encoding_with_transformers, create_article_id_to_value_mapping\n",
    "from utils import get_script_directory, slice_join_dataframes, truncate_history,sampling_strategy_wu2019, create_binary_labels_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_path = get_script_directory()\n",
    "\n",
    "DEFAULT_IS_BEYOND_ACCURACY_COL = \"is_beyond_accuracy\"\n",
    "\n",
    "\n",
    "PATH = Path(basic_path+\"/Data\")\n",
    "TRAIN_VAL_SPLIT = f\"ebnerd_demo\"  # [ebnerd_demo, ebnerd_small, ebnerd_large]\n",
    "TEST_SPLIT = f\"ebnerd_testset\"  # \"ebnerd_testset\", \"ebnerd_testset_gt\"\n",
    "\n",
    "#_____________________Training____________________________\n",
    "# Reads the behaviors file from training data\n",
    "df_behaviors_train = df_behaviors = pl.scan_parquet(\n",
    "    PATH.joinpath(TRAIN_VAL_SPLIT, \"train\", \"behaviors.parquet\")\n",
    ")\n",
    "# Reads the history file from training data\n",
    "df_history_train = df_behaviors = pl.scan_parquet(\n",
    "    PATH.joinpath(TRAIN_VAL_SPLIT, \"train\", \"history.parquet\")\n",
    ")\n",
    "\n",
    "#_____________________Validation____________________________\n",
    "# Reads the behaviors file from Validation data\n",
    "df_behaviors_val = df_behaviors = pl.scan_parquet(\n",
    "    PATH.joinpath(TRAIN_VAL_SPLIT, \"validation\", \"behaviors.parquet\")\n",
    ")\n",
    "# Reads the History file from Validation data\n",
    "df_history_val = df_behaviors = pl.scan_parquet(\n",
    "    PATH.joinpath(TRAIN_VAL_SPLIT, \"validation\", \"history.parquet\")\n",
    ")\n",
    "\n",
    "#_____________________Test____________________________\n",
    "# Reads the behaviors file from test data\n",
    "df_behaviors_test = df_behaviors = (\n",
    "    pl.scan_parquet(PATH.joinpath(TEST_SPLIT, \"test\", \"behaviors.parquet\"))\n",
    "    .filter(~pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "    .drop(DEFAULT_IS_BEYOND_ACCURACY_COL)\n",
    ")\n",
    "# Reads the History file from test data\n",
    "df_history_test = df_behaviors = pl.scan_parquet(\n",
    "    PATH.joinpath(TEST_SPLIT, \"test\", \"history.parquet\")\n",
    ")\n",
    "\n",
    "# ?? seem we already read this file ? but now without .drop(DEFAULT_IS_BEYOND_ACCURACY_COL)\n",
    "df_behaviors_test_ba = df_behaviors = pl.scan_parquet(\n",
    "    PATH.joinpath(TEST_SPLIT, \"test\", \"behaviors.parquet\")\n",
    ").filter(pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "\n",
    "#_____________________Reads Articles ??____________________________\n",
    "df_articles = pl.scan_parquet(PATH.joinpath(TEST_SPLIT,\"articles.parquet\")).collect()\n",
    "\n",
    "PLOT_PATH = Path(\"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom src.ebrec.utils._constants import (\\n    DEFAULT_HISTORY_ARTICLE_ID_COL, = f\"{\"article_id\"}_fixed\"\\n    DEFAULT_CLICKED_ARTICLES_COL, = \"article_ids_clicked\"\\n    DEFAULT_INVIEW_ARTICLES_COL, = \"article_ids_inview\"\\n    DEFAULT_IMPRESSION_ID_COL, = \"impression_id\"\\n    DEFAULT_SUBTITLE_COL, = \"subtitle\"\\n    DEFAULT_LABELS_COL, = \"labels\"\\n    DEFAULT_TITLE_COL, =  \"title\"\\n    DEFAULT_USER_COL, = \"user_id\"\\n)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "from src.ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL, = f\"{\"article_id\"}_fixed\"\n",
    "    DEFAULT_CLICKED_ARTICLES_COL, = \"article_ids_clicked\"\n",
    "    DEFAULT_INVIEW_ARTICLES_COL, = \"article_ids_inview\"\n",
    "    DEFAULT_IMPRESSION_ID_COL, = \"impression_id\"\n",
    "    DEFAULT_SUBTITLE_COL, = \"subtitle\"\n",
    "    DEFAULT_LABELS_COL, = \"labels\"\n",
    "    DEFAULT_TITLE_COL, =  \"title\"\n",
    "    DEFAULT_USER_COL, = \"user_id\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(\"user_id\", \"article_id_fixed\")\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=\"article_id_fixed\",\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=\"user_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_path = get_script_directory()\n",
    "\n",
    "PATH = Path(basic_path+\"/Data\")\n",
    "DATASPLIT = f\"ebnerd_demo\"  # [ebnerd_demo, ebnerd_small, ebnerd_large]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>list[i8]</td></tr></thead><tbody><tr><td>1927519</td><td>[9765955, 9767268, … 9763284]</td><td>[9772660, 9772768, … 9772750]</td><td>[9772660]</td><td>182127655</td><td>[1, 0, … 0]</td></tr><tr><td>1047632</td><td>[9768328, 9769367, … 9770541]</td><td>[9772168, 9772193, … 9772092]</td><td>[9772193]</td><td>255153419</td><td>[0, 1, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ u32     ┆ list[i32]         ┆ ---               ┆ ---              ┆ u32           ┆ list[i8]    │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 1927519 ┆ [9765955,         ┆ [9772660,         ┆ [9772660]        ┆ 182127655     ┆ [1, 0, … 0] │\n",
       "│         ┆ 9767268, …        ┆ 9772768, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9763284]          ┆ 9772750]          ┆                  ┆               ┆             │\n",
       "│ 1047632 ┆ [9768328,         ┆ [9772168,         ┆ [9772193]        ┆ 255153419     ┆ [0, 1, … 0] │\n",
       "│         ┆ 9769367, …        ┆ 9772193, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9770541]          ┆ 9772092]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMNS = [\n",
    "    \"user_id\",\n",
    "    \"article_id_fixed\",\n",
    "    \"article_ids_inview\",\n",
    "    \"article_ids_clicked\",\n",
    "    \"impression_id\",\n",
    "]\n",
    "HISTORY_SIZE = 10\n",
    "FRACTION = 0.01\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3037230</td><td>&quot;Ishockey-spiller: Jeg troede j…</td><td>&quot;ISHOCKEY: Ishockey-spilleren S…</td><td>2023-06-29 06:20:57</td><td>false</td><td>&quot;Ambitionerne om at komme til U…</td><td>2003-08-28 08:55:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sport/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Kendt&quot;, … &quot;Mindre ulykke&quot;]</td><td>142</td><td>[327, 334]</td><td>&quot;sport&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9752</td><td>&quot;Negative&quot;</td></tr><tr><td>3044020</td><td>&quot;Prins Harry tvunget til dna-te…</td><td>&quot;Hoffet tvang Prins Harry til a…</td><td>2023-06-29 06:21:16</td><td>false</td><td>&quot;Den britiske tabloidavis The S…</td><td>2005-06-29 08:47:00</td><td>[3097307, 3097197, 3104927]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[&quot;Harry&quot;, &quot;James Hewitt&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Kendt&quot;, … &quot;Personfarlig kriminalitet&quot;]</td><td>414</td><td>[432]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7084</td><td>&quot;Negative&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3037230   ┆ Ishockey- ┆ ISHOCKEY: ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9752    ┆ Negative │\n",
       "│           ┆ spiller:  ┆ Ishockey- ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ Jeg       ┆ spilleren ┆ 06:20:57  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ troede j… ┆ S…        ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3044020   ┆ Prins     ┆ Hoffet    ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7084    ┆ Negative │\n",
       "│           ┆ Harry     ┆ tvang     ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tvunget   ┆ Prins     ┆ 06:21:16  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ til       ┆ Harry til ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ dna-te…   ┆ a…        ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT,\"articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [\"subtitle\", \"title\"]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=\"article_id_fixed\",\n",
    "    eval_mode=False,\n",
    "    batch_size=32,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=\"article_id_fixed\",\n",
    "    eval_mode=True,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEWSENCODER - embedded layer: (None, 30, 768)\n",
      "WQ shape: (768, 400)\n",
      "NEWSENCODER - att 1: (None, 30, 400)\n",
      "NEWSENCODER - att 2: (None, 400)\n",
      "USERENCODER - His_in: (None, 10, 30)\n",
      "USERENCODER - Clicked: (None, 10, 400)\n",
      "WQ shape: (400, 400)\n",
      "USERENCODER - att 1: (None, 10, 400)\n",
      "USERENCODER - att 2: (None, 400)\n",
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 1.6222 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 1.5509 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 1.5074 - val_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 1.4758 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 1.4018 - val_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 1.3316 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 1.2659 - val_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 1.1665 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 1.1037 - val_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 0.9407 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "\n",
    "class hparams_nrms:\n",
    "    # INPUT DIMENTIONS:\n",
    "    title_size: int = 30\n",
    "    history_size: int = 50\n",
    "    # MODEL ARCHITECTURE\n",
    "    head_num: int = 20\n",
    "    head_dim: int = 20\n",
    "    attention_hidden_dim: int = 200\n",
    "    # MODEL OPTIMIZER:\n",
    "    optimizer: str = \"adam\"\n",
    "    loss: str = \"cross_entropy_loss\"\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 0.0001\n",
    "\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "MODEL_WEIGHTS = \"downloads/data/state_dict/NRMS/weights.weights.h5\"\n",
    "\n",
    "# CALLBACKS\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "#modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #filepath=MODEL_WEIGHTS, save_best_only=False, save_weights_only=True, verbose=1\n",
    "#)\n",
    "\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=10,\n",
    "    #callbacks=[tensorboard_callback, early_stopping], #, modelcheckpoint\n",
    ")\n",
    "#_ = model.model.load_weights(filepath=MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 868ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_validation = model.scorer.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_id', 'article_id_fixed', 'article_ids_inview', 'article_ids_clicked', 'impression_id', 'labels']\n"
     ]
    }
   ],
   "source": [
    "print(df_validation.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_train[\"user_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.5609728847930222,\n",
       "    \"mrr\": 0.351708404178917,\n",
       "    \"ndcg@5\": 0.3962766772528227,\n",
       "    \"ndcg@10\": 0.4768827168257954\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 77.95%\n",
      "0-Label Accuracy: 82.89%\n",
      "1-Label Accuracy: 23.05%\n"
     ]
    }
   ],
   "source": [
    "labels=df_validation[\"labels\"].to_list()\n",
    "predictions=df_validation[\"scores\"].to_list()\n",
    "\n",
    "flat_labels = [item for sublist in labels for item in sublist]\n",
    "flat_pred = [item for sublist in predictions for item in sublist]\n",
    "binary_values = [1 if prob > 0.5 else 0 for prob in flat_pred]\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = sum(t == p for t, p in zip(flat_labels, binary_values)) / len(flat_labels)\n",
    "\n",
    "# Accuracy for 0-valued labels\n",
    "zero_indices = [i for i, t in enumerate(flat_labels) if t == 0]\n",
    "zero_accuracy = sum(flat_labels[i] == binary_values[i] for i in zero_indices) / len(zero_indices)\n",
    "\n",
    "# Accuracy for 1-valued labels\n",
    "one_indices = [i for i, t in enumerate(flat_labels) if t == 1]\n",
    "one_accuracy = sum(flat_labels[i] == binary_values[i] for i in one_indices) / len(one_indices)\n",
    "\n",
    "print(f\"Overall Accuracy: {np.round(overall_accuracy*100,2):.2f}%\")\n",
    "print(f\"0-Label Accuracy: {np.round(zero_accuracy*100,2):.2f}%\")\n",
    "print(f\"1-Label Accuracy: {np.round(one_accuracy*100,2):.2f}%\")\n",
    "\n",
    "\n",
    "#______Pytorch softmax model results:_________\n",
    "#Average overall Accuracy: 68.18 %\n",
    "#Average 0-label Accuracy: 71.17 %\n",
    "#Average 1-label Accuracy: 32.6 %\n",
    "#________________________________________\n",
    "\n",
    "\n",
    "#______Pytorch sigmoid (weighted) model results:_________\n",
    "#Average overall Accuracy: 69.1 %\n",
    "#Average 0-label Accuracy: 72.8 %\n",
    "#Average 1-label Accuracy: 31.17 %\n",
    "#________________________________________\n",
    "\n",
    "\n",
    "#______Pytorch softmax - add  model results:_________\n",
    "#Average overall Accuracy: 51.87 %\n",
    "#Average 0-label Accuracy: 52.15 %\n",
    "#Average 1-label Accuracy: 50.86 %\n",
    "#________________________________________\n",
    "\n",
    "#______Pytorch softmax - ffnn  model results:_________\n",
    "#Average overall Accuracy: 90.99 %\n",
    "#Average 0-label Accuracy: 99.83 %\n",
    "#Average 1-label Accuracy: 0.0 %\n",
    "#________________________________________\n",
    "\n",
    "#______Pytorch sigmoid - ffnn  model results:_________\n",
    "#Average overall Accuracy: 55.81 %\n",
    "#Average 0-label Accuracy: 56.23 %\n",
    "#Average 1-label Accuracy: 50.97 %\n",
    "#________________________________________\n",
    "\n",
    "\n",
    "#______Pytorch sigmoid - add  model results:_________\n",
    "#Average overall Accuracy: 55.81 %\n",
    "#Average 0-label Accuracy: 56.23 %\n",
    "#Average 1-label Accuracy: 50.97 %\n",
    "#________________________________________\n",
    "\n",
    "#______Pytorch sigmoid - ffnn  model results:_________\n",
    "#Average overall Accuracy: 64.03 %\n",
    "#Average 0-label Accuracy: 67.22 %\n",
    "#Average 1-label Accuracy: 32.34 %\n",
    "#________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
